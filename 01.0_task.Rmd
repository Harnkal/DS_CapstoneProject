---
title: "Task 1: Getting and cleaning the data"
output: html_document
---

## Introduction and setup

This is a report for own use, so I can keep track of the advances made in the project. Each task will have its own section where all the objectives are achieved before advancing to the next task.

let's also load the workspace from last step.

```{r}
load("checkpoints/t0.RData")
```

importing all the libraries we are going to use in this task.

```{r message=FALSE, warning=FALSE}
library(doParallel)
library(dplyr)
library(quanteda)
```


## Execution

You may acess the description for this task [here](https://www.coursera.org/learn/data-science-project/supplement/IbTUL/task-1-getting-and-cleaning-the-data).

In this task we have to accomplish two tasks, tokenize our data and filter profanity from it

### Tokenization

The description of the task is: Identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.

The first thing we should do is to unify our data in a data frame to make it easier to work with. Let's apply the function  *to_df* to our data and remove the raw_data to save some memory. As we are here, let's also have a look at how much memory it is using.

```{r}
to_df <- function(lists, lang){
    ## Creating output data frame.
    out <- data.frame(ID = character(), 
                      Text = character(),
                      stringsAsFactors = FALSE)
    ## Transforming the data
    for(name in names(lists)){
        newdf <- data.frame(ID = paste(name, lang, 1:length(lists[[name]]), sep = "."),
                            stringsAsFactors = FALSE)
        newdf["Text"] <- as.character(lists[[name]])
        ## Merging data from different sources
        out <- bind_rows(out, newdf)
    }
    return(data.frame(out))
}

df_data <- to_df(raw_data, lang = "en_US")
rm(raw_data)
cat(sprintf("df_data is occupying %.2fMBs in memory", object.size(df_data)/1024/1024))
```

The quanteda package should help us with the tokenization. Let's first tokenize the texts as they are so we can have an idea of what they look like. This is a very long process so I decided to parallelize it. It take quite a toll on my computer and the logistics are still heavy, but I've been able to reduce the time in 50%.

```{r}
ptokens <- function(docs, dnames, tolower = FALSE, ...){
    ## tolower
    if(tolower) docs <- tolower(docs)
    ## Tokenizing
    toks <- tokens(docs, ...)
    names(toks) <- dnames
    ##Returning output
    return(toks)
}

system.time(raw_tokens <- ptokens(docs=df_data$Text, dnames=df_data$ID, what = "word"))
```

So here is a sample of what the tokenized data look like vs. the raw text data. We are not using it so I'm deleting it.

```{r}
cat(paste("Raw text data Sample:\n", df_data$Text[1],"\n\n"))
print(as.list(raw_tokens[1]))
rm(raw_tokens)
```

This is a lot of data, but not everything in it is useful. As the objective here is to create a recommendation system for words, there are some things that we might not want to predict like numbers, punctuation, symbols, hyphens and urls. So we are going to remove those from our tokens. We are also sending everything to lowercase because I don't think I have the knowledge or the data to make this case sensitive.

```{r}
system.time(f_tokens <- ptokens(docs=df_data$Text, dnames=df_data$ID, tolower=TRUE, 
                                what = "word", remove_punct = TRUE, 
                                remove_symbols = TRUE, remove_numbers = TRUE, 
                                remove_url = TRUE, remove_separators=TRUE,
                                split_hyphens = TRUE))
```

Let's take a look at how the data look like now

```{r}
cat(paste("Raw text data Sample:\n", df_data$Text[1],"\n\n"))
print(as.list(f_tokens[1]))
```

This looks as close to perfection as we will get. Now, on to the profanity filtering.

### Profanity filtering

We want to make a product that can be used by people of all ages, so we need to remove profanity from the set of words that can be predicted. And that is what we are going to do now.

Now let's download a list of profanities that I found on github and read it to the workspace.

```{r}
## Define profanity file path
file_path <- "data/final/en_US/profanity.txt"
## Download if not already downloaded
if(!file.exists(file_path)){
    url <- "https://raw.githubusercontent.com/shutterstock/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
    download.file(url, destfile = file_path, method = "curl")
}
## Reads profanity file into memory
profanity_list <- read.csv(file_path, header = FALSE, stringsAsFactors = FALSE)
profanity_list <- profanity_list$V1
```

Now in 2022, with a little more experience, I noticed that some profanity are not made out of a single word, so filtering it after the tokenization is dangerous, we don't want kids to write "ball" and receive the recommendation "sack" even though none of these words are a problem on their own. More than that, we do have a lot of documents, which means, it is not going to be a problem to remove the ones that contain offensive words or expressions instead of removing an arbitrary word or expression from it. This way we guaranty that the documents that are staying are going to make sense. So I created a new function called *filterProfDocs*, as in filter profane documents. This should take a long time but I've been able to parallelize it quite well.

```{r, cache=TRUE, cache.lazy=FALSE}
filterProfDocs <- function(docs, plist) {
    nc <- detectCores()-1
    cl = makePSOCKcluster(nc)
    registerDoParallel(cl)
    
    suppressWarnings(pchunks <- split(plist, 1:nc))
    
    filterLists <- foreach(pchunk = pchunks, .combine = "c") %dopar% {
        pattern = paste("\\b",pchunk,"\\b", sep = "",collapse = "|")
        list(grepl(pattern, docs, ignore.case = TRUE))
    }
    stopCluster(cl)
    filterLists <- matrix(unlist(filterLists), ncol = nc)
    filter = rowSums(filterLists) == 0
    return(filter)
}

system.time(pfilter <- filterProfDocs(df_data$Text, profanity_list))
```

Let's have a look at how many documents are leaving our corpus

```{r}
cat(sprintf("%i documents are being removed from our copus\n%i documents remaining \nthe removed amount corresponds to %.2f%% of the documents",
            sum(!pfilter), sum(pfilter), mean(!pfilter)*100))
```

Great!! Let's update our data and tokenize it again.

```{r}
df_data <- filter(df_data, pfilter)
system.time(f_tokens <- ptokens(docs=df_data$Text, dnames=df_data$ID, tolower=TRUE, 
                                what = "word", remove_punct = TRUE, 
                                remove_symbols = TRUE, remove_numbers = TRUE, 
                                remove_url = TRUE, remove_separators=TRUE,
                                split_hyphens = TRUE))
```
Let's have a look at the first doc again, just for the sake of it.

```{r}
cat(paste("Raw text data Sample:\n", df_data$Text[1],"\n\n"))
print(as.list(f_tokens[1]))
```
Perfect, with that, let's head on to the next task.

Saving workspace.

```{r}
rm(list = c("file_path", "pfilter", "profanity_list", "filterProfDocs", "ptokens", "to_df"))
save.image("checkpoints/t1.RData")
```