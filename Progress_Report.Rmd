---
title: "Progress Report"
author: "Silva, Rafael"
date: "June 11, 2019"
output:
  html_document:
    keep_md: yes
  pdf_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a report for own use, so I can keep track of the advances made in the project. Each task will have its own section where all the objectives are achieved before advancing to the next task.

## Setup

Here are the packages used for this report

```{r, message=FALSE}
library(dplyr)
library(quanteda)
#quanteda_options("threads"=2)
library(doParallel)
```

## Task 0: Undertanding the problem

You may acess the description for this task [here](https://www.coursera.org/learn/data-science-project/supplement/Iimbd/task-0-understanding-the-problem).

As the general idea of this task is to download and read the data in the work environment (WE), I created 2 functions to do exactly that.

```{r}
source("functions/t0_functions.R")
```

The function *get.data()* accomplish the task of downloading and extracting the data into the working directory (if not already downloaded).

```{r}
get.data()
```

The *read.data()* function reads the *n* first lines of the files for the language and source selected. As the computer I'm working on is dealing very well with the size of the files for one language only, I will read the files entirely.

```{r}
raw_data <- read.data(lang = "en_US", s = c("blogs", "news", "twitter"), n=-1L)
```

Let's take a look at how the data looks like

```{r}
print(paste("blogs has", length(raw_data$blogs), "observations"))
print(paste("news has", length(raw_data$news), "observations"))
print(paste("twitter has", length(raw_data$twitter), "observations"))
```

```{r}
cat(paste("first line of blogs:\n", raw_data$blogs[1]))
cat(paste("\n\nfirst line of news:\n", raw_data$news[1]))
cat(paste("\n\nfirst line of twitter:\n", raw_data$twitter[1]))
```

Right now we have an idea how of how the data looks like, in the next steps we might jump into the data cleaning and preparation.

All the functions created in this taks are in **"functions/t0_functions.R"**.

## Task 1: Getting and cleaning the data

You may acess the description for this task [here](https://www.coursera.org/learn/data-science-project/supplement/IbTUL/task-1-getting-and-cleaning-the-data).

In this task we have to accomplish two tasks, tokenize our data and filter profanity from it.

### Tokenization

The description of the task is: Identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.

The first thing we should do is to unify our data in a data frame to make it easier to work with, but without losing any information. Here is a function to do that.

```{r}
source("functions/t1_functions.R")
```

Let's apply it to our data and remove the raw_data to save some memory

```{r}
df_data <- to_df(raw_data, lang = "en_US")
rm(raw_data)
```

The quanteda package should help us with the tokenization. Let's first tokenize the texts as they are so we can have an idea of what they look like. This is a very long process so I decided to parallelize it. It take quite a toll on my computer and the logistics are still havy, but I've been able to reduce the time in 50%.

```{r}
init <- Sys.time()
raw_tokens <- ptokens(docs=df_data$Text, dnames=df_data$ID, ncl=7, what = "word")
Sys.time()-init
```

So here is a sample of what the tokenized data look like vs. the raw text data. We are not using it so i'm deleting it.

```{r}
cat(paste("Raw text data Sample:\n", df_data$Text[1],"\n\n"))
print(raw_tokens[1])
rm(raw_tokens)
```

This is a lot of data, but not everything in it is useful. As the objective here is to create a recomendation system for words, There are some things that we might not want to predict like numbers, punctuations, symbols, hyphens and urls. So we are going to remove those from our tokens. We are also sending everything to lowercase because I don't think I have the knowledge or the data to make this case sensistive.

```{r}
init <- Sys.time()
f_tokens <- ptokens(docs=df_data$Text, dnames=df_data$ID, ncl=7, tolower=TRUE, 
                    what = "word", remove_numbers = TRUE, remove_punct = TRUE,
                    remove_symbols = TRUE, remove_twitter = FALSE, remove_hyphens = TRUE,
                    remove_url = TRUE)
Sys.time()-init
```

Let's take a look at how the look like now

```{r}
cat(paste("Raw text data Sample:\n", df_data$Text[1],"\n\n"))
print(f_tokens[1])
```

Well, it is much cleaner, but we just want actual words in our vocabulary. There are still things like `r f_tokens[[1]][16] and f_tokens[[1]][17]`. We must find a way of addressing that.

```{r}

```







