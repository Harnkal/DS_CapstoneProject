---
title: "Progress Report"
author: "Silva, Rafael"
date: "June 11, 2019"
output:
  html_document:
    keep_md: no
  pdf_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a report for own use, so I can keep track of the advances made in the project. Each task will have its own section where all the objectives are achieved before advancing to the next task.

Each task will be treated as a sepparated report. It means that in each task the data will be loaded and prepared again, this allows me to run each task sepparetely. Also, this is going to make the report slowier to run in its entirety.

## Task 0: Undertanding the problem

You may acess the description for this task [here](https://www.coursera.org/learn/data-science-project/supplement/Iimbd/task-0-understanding-the-problem).

As the general idea of this task is to download and read the data in the work environment (WE), I created 2 functions to do exactly that.

```{r}
# t0_functions

## This script has all the functions created to accomplish the task 0, they will
## be used in the other scripts and reports


# get.data
## used to download and extract the datasets into the data directory.
get.data <- function() {
    ## Downloading and allocating
    if(!dir.exists("data")) dir.create("data")
    if(!file.exists("data/Coursera-SwiftKey.zip")) {
        download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",
                      "data/Coursera-SwiftKey.zip")
    }
    if(!dir.exists("data/final")) unzip("data/Coursera-SwiftKey.zip", exdir = "data")
}

# read.data
## Used to read the project data into R. It takes the following arguments:
##   - lang: the language in which you want to read that can be "en_US", "de_DE", 
##           "fi_FI" and "ru_RU" for English, German, Finnish and Russian 
##           respectively.
##   - source: from which source you want to read you can choose one or more from
##             "blogs", "news"and "twitter".
##   - n: The number of lines to read from each file. n = -1L means that all the 
##        lines should be read.
## This function returns a list that contains a list element for each of the read
## files.
read.data <- function(lang="en_US", Source=c("blogs", "news", "twitter"), n=-1L) {
    directory <- paste("./data/final/", lang, "/", sep="")
    out = list()
    for (i in 1:length(Source)) {
        con <- file(paste(directory, lang, ".", Source[i], ".txt", sep=""))
        out[[Source[i]]] <- readLines(con, n=n, warn=FALSE)
        close(con)
    }
    return(out)
} 
```

The function *get.data()* accomplish the task of downloading and extracting the data into the working directory (if not already downloaded).

```{r}
get.data()
```

The *read.data()* function reads the *n* first lines of the files for the language and source selected. As the computer I'm working on is dealing very well with the size of the files for one language only, I will read the files entirely.

```{r}
raw_data <- read.data(lang = "en_US", source = c("blogs", "news", "twitter"), n=-1L)
```

Let's take a look at how the data looks like

```{r}
print(paste("blogs has", length(raw_data$blogs), "observations"))
print(paste("news has", length(raw_data$news), "observations"))
print(paste("twitter has", length(raw_data$twitter), "observations"))
```

```{r}
cat(paste("first line of blogs:\n", raw_data$blogs[1]))
cat(paste("\n\nfirst line of news:\n", raw_data$news[1]))
cat(paste("\n\nfirst line of twitter:\n", raw_data$twitter[1]))
```

Right now we have an idea how of how the data looks like, in the next steps we might jump into the data cleaning and preparation.

## Task 1: Getting and cleaning the data

You may acess the description for this task [here](https://www.coursera.org/learn/data-science-project/supplement/IbTUL/task-1-getting-and-cleaning-the-data).

Before starting this task we need to read the data into the WE.

```{r}
source("functions/t0_functions.R")
get.data()
raw_data <- read.data()
```

In this task we have to accomplish two tasks, tokenize our data and filter profanity from it.

### Tokenization

The description of the task is: Identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it.











